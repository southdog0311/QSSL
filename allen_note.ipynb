{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import moco.builder\n",
    "import moco.loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "train_dataset.data: <class 'numpy.ndarray'>\n",
      "train_labels: [6 9 9 ... 9 1 1]\n",
      "train_idx: [   29    30    35 ... 49963 49971 49997]\n",
      "train_idx.size: 50000\n",
      "train_dataset.targets: [0 0 0 ... 9 9 9]\n",
      "train_dataset.targets.size: 50000\n",
      "train_dataset.data.size: 153600000\n",
      "train_dataset.data.shape: (50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 dataset (try train_simclr.py code)\n",
    "\n",
    "datadir = './data'\n",
    "classes = 10\n",
    "epoch_size = 50000\n",
    "# Normalization for CIFAR\n",
    "normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                                 std=[0.2023, 0.1994, 0.2010])\n",
    "\n",
    "augmentation = [\n",
    "    transforms.RandomResizedCrop(32, scale=(0.2, 1.0)),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # not strengthened\n",
    "    ], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    # transforms.RandomApply([moco.loader.GaussianBlur([.1, 2.])], p=0.5),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "]\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=datadir, train=True, download=True,\n",
    "                                 transform=moco.loader.TwoCropsTransform(\n",
    "                                     transforms.Compose(augmentation)))\n",
    "\n",
    "print(f'train_dataset.data: {type(train_dataset.data)}')\n",
    "\n",
    "\n",
    "train_labels = np.array(train_dataset.targets)\n",
    "print(f'train_labels: {train_labels}')\n",
    "num_classes = classes\n",
    "train_idx = np.array(\n",
    "    [np.where(train_labels == i)[0][:int(epoch_size / num_classes)] for i in range(0, num_classes)]).flatten()\n",
    "train_dataset.targets = train_labels[train_idx]\n",
    "train_dataset.data = train_dataset.data[train_idx]\n",
    "print(f'train_idx: {train_idx}')\n",
    "print(f'train_idx.size: {train_idx.size}')\n",
    "print(f'train_dataset.targets: {train_dataset.targets}')\n",
    "print(f'train_dataset.targets.size: {train_dataset.targets.size}')\n",
    "print(f'train_dataset.data.size: {train_dataset.data.size}')\n",
    "print(f'train_dataset.data.shape: {train_dataset.data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "train_idx.size: 50000\n",
      "train_idx: [   29    30    35 ... 49963 49971 49997]\n",
      "train_dataset.data: (50000, 32, 32, 3)\n",
      "train_dataset.data: <class 'numpy.ndarray'>\n",
      "train_dataset.targets: (50000,)\n",
      "train_dataset.targets: <class 'numpy.ndarray'>\n",
      "train_dataset.targets: 50000\n"
     ]
    }
   ],
   "source": [
    "# CiFAR-10 (try linear_probe_simclr.py code)\n",
    "\n",
    "datadir = './data'\n",
    "classes = 10\n",
    "\n",
    "# For CIFAR\n",
    "normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                                 std=[0.2023, 0.1994, 0.2010])\n",
    "\n",
    "augmentation = [\n",
    "    transforms.RandomResizedCrop(32),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # not strengthened\n",
    "    ], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    # transforms.RandomApply([moco.loader.GaussianBlur([.1, 2.])], p=0.5),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "]\n",
    "\n",
    "num_classes = classes\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=datadir, train=True, download=True,\n",
    "                                 transform=transforms.Compose(augmentation))\n",
    "\n",
    "# print(f'train_dataset.targets: {train_dataset.targets.shape}')\n",
    "# print(f'train_dataset.data: {train_dataset.data.shape}')\n",
    "train_labels = np.array(train_dataset.targets)\n",
    "# print(train_labels.size)\n",
    "# print(train_dataset.targets)\n",
    "train_idx = np.array(\n",
    "    [np.where(train_labels == i)[0] for i in range(0, num_classes)]).flatten()\n",
    "print(f'train_idx.size: {train_idx.size}')\n",
    "print(f'train_idx: {train_idx}')\n",
    "train_dataset.targets = train_labels[train_idx]\n",
    "train_dataset.data = train_dataset.data[train_idx]\n",
    "print(f'train_dataset.data: {train_dataset.data.shape}')\n",
    "print(f'train_dataset.data: {type(train_dataset.data)}')\n",
    "# print(f'train_dataset.data: {train_dataset.data}')\n",
    "print(f'train_dataset.targets: {train_dataset.targets.shape}')\n",
    "print(f'train_dataset.targets: {type(train_dataset.targets)}')\n",
    "print(f'train_dataset.targets: {train_dataset.targets.size}')\n",
    "# print(f'train_dataset.targets: {train_dataset.targets}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023-3-12 custom dataset created by Allen LIN\n",
    "\n",
    "class fingerprintDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.targets = self.img_labels.iloc[:, 1] # label of the dataset\n",
    "        self.target_transform = target_transform\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[0, 0])\n",
    "        image = cv2.imread(img_path)\n",
    "        self.data = np.empty((len(self.img_labels), *image.shape), dtype=np.uint8)\n",
    "        for i in range(len(self.img_labels)):\n",
    "            self.data[i] = image\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = cv2.imread(img_path)\n",
    "        print(image.shape)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103, 96, 3)\n",
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# SOCOFing fingerprint dataset (try linear_probe_simclr.py code)\n",
    "\n",
    "img_dir = os.path.join(\"../\", \"kaggle_fingerprint\", \"SOCOFing\", \"All\")\n",
    "annotations_file = os.path.join(\"../\", \"kaggle_fingerprint\", \"kaggle_fingerprint_annotations.csv\")\n",
    "classes = 600\n",
    "epoch_size = 55270\n",
    "\n",
    "# For CIFAR\n",
    "normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                                 std=[0.2023, 0.1994, 0.2010])\n",
    "\n",
    "augmentation = [\n",
    "    transforms.ToPILImage(), # to PIL format\n",
    "    transforms.RandomResizedCrop(32),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # not strengthened\n",
    "    ], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    # transforms.RandomApply([moco.loader.GaussianBlur([.1, 2.])], p=0.5),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "]\n",
    "\n",
    "train_dataset = fingerprintDataset(annotations_file, img_dir, transform=transforms.Compose(augmentation))\n",
    "\n",
    "img, label = train_dataset[0]\n",
    "print(img.shape)\n",
    "\n",
    "# train_labels = np.array(train_dataset.targets)\n",
    "\n",
    "# train_idx = np.array(\n",
    "#     [np.where(train_labels == i)[0] for i in range(0, num_classes+1)]).flatten()\n",
    "\n",
    "# train_idx = np.array(\n",
    "#         [np.where(train_labels == i)[0][:int(epoch_size / num_classes)+1] for i in range(0, num_classes+1)], dtype=object).flatten()\n",
    "\n",
    "# train_idx = np.hstack(train_idx)\n",
    "# print(f'train_idx.size: {train_idx.size}')\n",
    "# # print(f'train_idx: {train_idx}')\n",
    "# train_dataset.targets = train_labels[train_idx]\n",
    "# train_dataset.data = train_dataset.data[train_idx]\n",
    "# print(f'train_dataset.data: {train_dataset.data.shape}')\n",
    "# print(f'train_dataset.data: {type(train_dataset.data)}')\n",
    "# # print(f'train_dataset.data: {train_dataset.data}')\n",
    "# print(f'train_dataset.targets: {train_dataset.targets.shape}')\n",
    "# print(f'train_dataset.targets: {type(train_dataset.targets)}')\n",
    "# print(f'train_dataset.targets: {train_dataset.targets.size}')\n",
    "# # print(f'train_dataset.targets: {train_dataset.targets}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
